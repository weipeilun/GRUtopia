from typing import List
import numpy as np
import torch
import time
from omni.isaac.core.articulations import ArticulationSubset
from omni.isaac.core.utils.types import ArticulationAction
from omni.isaac.core.scenes import Scene
from omni.isaac.sensor import Camera
from grutopia_extension.models.simple_policy import SimplePolicy

import grutopia.core.util.gym as gymutil
import grutopia.core.util.math as math_utils
from grutopia.core.util.interaction import BaseInteraction
from grutopia.core.robot.controller import BaseController
from grutopia.core.robot.robot import BaseRobot
from grutopia.core.robot.robot_model import ControllerModel
from grutopia.core.constants import OBSERVATION_TRACKING_COMMAND_KEY, JOINT_ACTION_KEY
from grutopia.core.util.rsl_rl import pickle
from grutopia_extension.training.ppo.ppo import PPO
from grutopia_extension.training.ppo.actor_critic import ActorCritic
from grutopia_extension.training.utils import features as features_util


@BaseController.register('HumanoidRandomMoveController')
class HumanoidRandomMoveController(BaseController):
    """Controller class converting locomotion speed control action to joint positions for H1 robot."""
    """
    joint_names_sim and joint_names_gym define default joint orders in isaac-sim and isaac-gym.
    """

    def __init__(self, config: ControllerModel, robot: BaseRobot, scene: Scene) -> None:
        super().__init__(config=config, robot=robot, scene=scene)

        self.keyboard: BaseInteraction = BaseInteraction.interactions['Keyboard']()

        self.policy = self.load_policy(config)
        self._old_joint_positions = np.zeros(19)
        self.policy_input_obs_num = 350
        self._old_policy_obs = np.zeros(self.policy_input_obs_num)
        self._apply_times_left = 0  # Specifies how many times the action generated by the policy needs to be repeatedly applied.

        self.applied_joint_positions = np.zeros(19, dtype=np.float32)

        self.default_privileged_obs_buf = torch.zeros((1, 18), device='cpu',
                                              dtype=torch.float, requires_grad=False)

    def load_policy(self, config):
        body = torch.jit.load(f'{config.policy_weights_path}/body_{config.policy_weights_iter}.jit')
        # adaptation_module = torch.jit.load(f'{config.policy_weights_path}/adaptation_module_{config.policy_weights_iter}.jit')
        env_factor_encoder_module = torch.jit.load(f'{config.policy_weights_path}/env_factor_encoder_{config.policy_weights_iter}.jit')

        def policy(obs, obs_history):
            with torch.inference_mode():
                # latent = adaptation_module.forward(obs_history.to('cpu'))
                latent = env_factor_encoder_module.forward(obs_history.to('cpu'))
                action = body.forward(torch.cat((obs.to('cpu'), latent), dim=-1))
            return action

        return policy

    def forward(
            self,
            forward_speed,
            rotation_speed,
            lateral_speed
    ) -> ArticulationAction:
        if self._apply_times_left > 0:
            self._apply_times_left -= 1
            return ArticulationAction(joint_positions=self.applied_joint_positions)

        tracking_command = np.array([forward_speed, lateral_speed, rotation_speed], dtype=np.float32)

        original_obs_dict = self.robot.get_obs(training=False)
        original_obs_dict[OBSERVATION_TRACKING_COMMAND_KEY] = tracking_command
        original_obs_dict[JOINT_ACTION_KEY] = self._old_joint_positions
        original_obs_array = features_util.observation_dict_to_feature_array(original_obs_dict)
        original_obs_tensor = torch.Tensor(original_obs_array, device='cpu')
        self._old_policy_obs = np.concatenate([self._old_policy_obs[70:350], original_obs_array])
        obs_history_tensor = torch.Tensor(self._old_policy_obs, device='cpu')

        actions = self.policy(original_obs_tensor.reshape((1, -1)), self.default_privileged_obs_buf)
        joint_positions = actions[0]

        # obs_left_ankle_pos = original_obs_dict[OBSERVATION_LEFT_ANKLE_POS_KEY]
        # obs_right_ankle_pos = original_obs_dict[OBSERVATION_RIGHT_ANKLE_POS_KEY]
        # # Add a small step.
        # step_error = np.abs(np.abs(obs_left_ankle_pos[2] - obs_right_ankle_pos[2]) - 0.15)
        # step_reward = np.exp(-step_error**2 / 0.05) * 1.  # Scale reduced

        self._old_joint_positions = joint_positions.numpy()
        self.applied_joint_positions = joint_positions
        self._apply_times_left = self.robot.user_config.per_inference_frames - 1

        return ArticulationAction(joint_positions=self.applied_joint_positions)

    def action_to_control(self, action: List | np.ndarray) -> ArticulationAction:
        """Convert input action (in 1d array format) to joint positions to apply.

        Args:
            action (List | np.ndarray): 3-element 1d array containing:
              0. forward_speed (float)
              1. lateral_speed (float)
              2. rotation_speed (float)

        Returns:
            ArticulationAction: joint positions to apply.
        """
        assert len(action) == 0, 'action must contain 3 elements'

        key_input = self.keyboard.get_input()
        if len(key_input) != 6:
            raise RuntimeError(f'Please update the keyboard interactive. The key_input is {key_input}')
        forward_speed = 0
        lateral_speed = 0
        rotation_speed = 0

        if key_input[0] == 1:
            forward_speed = 1
        elif key_input[1] == 1:
            forward_speed = -1
        elif key_input[2] == 1:
            lateral_speed = 1
        elif key_input[3] == 1:
            lateral_speed = -1
        elif key_input[4] == 1:
            rotation_speed = 1
        elif key_input[5] == 1:
            rotation_speed = -1

        return self.forward(forward_speed=forward_speed,
                                               rotation_speed=rotation_speed,
                                               lateral_speed=lateral_speed)
